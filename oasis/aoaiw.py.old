import numpy as np
import pandas as pd
from scipy.special import expit
from sklearn.cluster import KMeans
import copy
import warnings

def stratify_by_features(features, num_st, scores = None,
                            split_threshold = None, n_init = 10, n_jobs = 1):
    """
    Input
    -----
    features : float (n x m) numpy array
        feature matrix where rows correspond to data points and columns
        correspond to features.

    num_st : integer
        number of strata to create.

    scores : float numpy array of length n, optional, default None
        array of scores corresponding to each row in the feature matrix. Only
        required if split_threshold is not None.

    split_threshold : float, optional, default None
        separate the points into two groups -- predicted positives and
        predicted negatives -- according to whether the score is above or
        below `split_threshold`. Then stratify separately within each group.
        This option requires `scores` to be present.

    n_init : integer, optional, default 10
        number of times to run K-Means with different centroid seeds

    n_jobs : integer, optional, default 1
        number of jobs (i.e. number of CPUs) to use for the K-Means method

    Output
    ------
    stratum_ids : integer numpy array of length n
        contains the stratum index corresponding to each data point
    """

    n_pts = features.shape[0]

    stratum_ids = np.empty(n_pts, dtype='int')

    if split_threshold is not None:
        if type(scores) is not np.ndarray:
            raise TypeError("scores must be a numpy.ndarray")
        if len(scores) != n_pts:
            raise ValueError("length of scores does not match shape of features")
        if ( split_threshold > np.max(scores) or
                        split_threshold < np.min(scores) ) :
            raise ValueError("split_threshold is outside the range of scores")

        pos_pts = scores >= split_threshold
        n_pos_pts = np.sum(pos_pts)
        n_neg_pts = n_pts - n_pos_pts
        n_pos_st = np.ceil(n_pos_pts/n_pts * num_st).astype(int)
        n_neg_st = num_st - n_pos_st

        # Cluster positive pts
        km = KMeans(n_clusters = n_pos_st, n_init = n_init, n_jobs = n_jobs)
        stratum_ids[pos_pts] = km.fit_predict(X = features[pos_pts,:])

        # Cluster negative pts
        km = KMeans(n_clusters = n_neg_st, n_init = n_init, n_jobs = n_jobs)
        stratum_ids[~pos_pts] = km.fit_predict(X = features[~pos_pts,:]) + n_pos_st
    else:
        # Cluster all pts together
        km = KMeans(n_clusters = num_st, n_init = n_init, n_jobs = n_jobs)
        stratum_ids = km.fit_predict(X = features)

    return stratum_ids

def stratify_by_scores(scores, goal_num_st, method = 'equal_size',
                       split_threshold = None):
    """
    Input
    -----
    scores : float numpy array of length n
        array containing scores for each data point.

    goal_num_st : integer
        number of strata to create. For some methods, this number is
        a goal -- it is not guaranteed. The actual number of strata created is
        returned as output.

    method : 'cum_sqrt_F' or 'equal_size', optional, default 'equal_size'

    split_threshold : float, optional, default None
        separate the points into two groups -- predicted positives and
        predicted negatives -- according to whether the score is above or
        below `split_threshold`. Then stratify by scores separately within
        each group. (Note: not implemented for `cum_sqrt_F` yet)

    Output
    ------
    stratum_ids : integer numpy array of length n
        contains the stratum index corresponding to each data point

    num_st : integer
        the actual number of strata (may be different from `goal_num_st`)
    """
    def alloc_equally(amount, number):
        quotient = amount // number
        remainder = amount % number

        allocated_amount = [quotient for i in range(number)]
        for i in range(remainder):
            allocated_amount[i] += 1
        return allocated_amount

    if method == 'equal_size':

        n_pts = len(scores)
        stratum_ids = np.empty(n_pts, dtype='int')

        if split_threshold is not None:
            if ( split_threshold > np.max(scores) or
                            split_threshold < np.min(scores) ) :
                raise ValueError("split_threshold is outside the range of scores")

            pos_pts = scores >= split_threshold
            n_pos_pts = np.sum(pos_pts)
            n_neg_pts = n_pts - n_pos_pts
            n_pos_st = np.ceil(n_pos_pts/n_pts * goal_num_st).astype(int)
            n_neg_st = goal_num_st - n_pos_st

            st_pops_pos = alloc_equally(n_pos_pts, n_pos_st)
            st_pops_neg = alloc_equally(n_neg_pts, n_neg_st)

            st_pops = np.concatenate((st_pops_neg, st_pops_pos))
        else:
            st_pops = np.array(alloc_equally(n_pts, goal_num_st))

        sort_ids = np.argsort(scores)

        i = 0
        for k,nk in enumerate(st_pops):
            start = i
            end = i + nk
            stratum_ids[sort_ids[start:end]] = k
            i = end

        num_st = goal_num_st

    # FIXME consider removing this option or implementing split_threshold
    if method == 'cum_sqrt_F':
        # approx distribution of scores -- called f
        counts, score_bins = np.histogram(scores, bins=goal_num_st**2)

        # generate cumulative dist of sqrt(f)
        sqrt_counts = np.sqrt(counts)
        cum_sq_f = np.cumsum(sqrt_counts)

        # equal bin width on cum sqrt(f) scale
        width_cum_sq_f = np.sum(cum_sq_f)/goal_num_st

        # construct score bins so that they create equal intervals on the
        # cum sqrt(f) scale.
        new_bins = []
        i = 0
        while i < len(cum_sq_f) - 1:
            new_bins.append(score_bins[i])
            sum = cum_sq_f[i]
            for j in range(i + 1, len(cum_sq_f)):
                sum = sum + cum_sq_f[j]
                if sum >= width_cum_sq_f or j == (len(cum_sq_f) - 1):
                    # finish bin
                    if j != (len(cum_sq_f) - 1) and len(new_bins)%2 == 0:
                        # every second bin has width smaller than width_cum_sq_f
                        sum = sum - cum_sq_f[j]
                        i = j
                    else:
                        i = j + 1
                    break
        new_bins.append(score_bins[len(score_bins) - 1])
        new_bins = np.array(new_bins)
        new_bins[0] -= 0.01

        # calculate stratum ids
        stratum_ids = np.digitize(scores, bins = new_bins, right = True) - 1

        # remove stratum ids with population zero
        palette = np.unique(stratum_ids)
        num_st = len(palette)
        key = np.arange(num_st)
        stratum_ids = np.digitize(stratum_ids, palette, right=True)

    return (stratum_ids, num_st)

class Strata:
    """
    Represents a collection of strata, which contain data points. The data
    points are referenced by a unique index. Information about the data points
    must be stored elsewhere.

    Input
    -----
    stratum_ids :
        an integer array containing the stratum index for the datapoint at the
        corresponding location in `indices`. Note that the stratum indices take
        values in {0, 1, ..., K - 1}, where K is the number of strata.

    scores : float numpy array of length n

    score_threshold :

    calibrated_score : boolean
        whether the scores given are calibrated probabilities

    preds : float numpy array of length n, optional, default None
        use when preds != (scores >= score_threshold)*1

    splitting : bool, optional, default False
        whether to split stratum when...
    """
    def __init__(self, stratum_ids, scores, score_threshold,
                 calibrated_score, preds = None, splitting = False):

        # Total number of points
        self.num_pts = len(stratum_ids)

        ids = np.arange(self.num_pts)

        # Store stratum indices
        self.indices = np.unique(stratum_ids)

        self.num_st = len(self.indices)

        # Store datapoint indices by stratum
        self.allocations = []
        for k in self.indices:
            self.allocations.append(ids[np.where(stratum_ids == k)])

        # Store population for each stratum
        self.populations = np.array([len(x) for x in self.allocations])

        # Store scores
        self.scores = []
        for k in self.indices:
            self.scores.append(scores[self.allocations[k]])

        # Store predictions
        if preds is None:
            preds = (scores >= score_threshold)*1
        self.preds = []
        for k in self.indices:
            self.preds.append(preds[self.allocations[k]])

        # Sort points within strata in ascending order of score
        for k in self.indices:
            sorted_id = np.argsort(self.scores[k])
            self.scores[k] = self.scores[k][sorted_id]
            self.allocations[k] = self.allocations[k][sorted_id]

        self.score_threshold = score_threshold
        self.calibrated_score = calibrated_score
        self.splitting = splitting

        # Calculate weights
        self.weights = self.populations/self.num_pts

        # Store sampled labels
        self.labels = [np.repeat(np.nan, x) for x in self.populations]

    def _add_stratum(self):
        """
        Add a new empty stratum
        """
        new_stratum_id = np.max(self.indices) + 1
        self.indices = np.append(self.indices, new_stratum_id)
        self.num_st += 1
        self.allocations.append(np.array([], dtype='int'))
        self.populations = np.append(self.populations, 0)
        self.scores.append(np.array([], dtype='float'))
        self.preds.append(np.array([], dtype='float'))
        self.weights = np.append(self.weights, 0)
        self.labels.append(np.array([], dtype='int'))

        return new_stratum_id

    def _sample_stratum(self, prob_dist = None):
        """
        Sample from the stratum indices

        Input
        -----
        prob_dist : float numpy array of length K (number of strata), optional
            probability distribution to use when sampling from the strata. If
            None, use the stratum weights.

        Output
        ------
        a randomly selected stratum index in the set {0, 1, ..., K - 1}
        """
        if prob_dist is None:
            # Use weights
            prob_dist = self.weights

        return np.random.choice(self.indices, p = prob_dist)

    def _sample_in_stratum(self, stratum_id, replace = True):
        """
        Sample a datapoint uniformly from a stratum

        Input
        -----
        stratum_id : integer
            stratum index to sample from

        replace : bool, optional, default True
            whether to sample with or without replacement

        Output
        ------
        a randomly selected location in the stratum
        """

        if replace:
            points = self.populations[stratum_id]
        else:
            # Sample only from points which haven't been seen
            points = np.arange(self.populations[stratum_id])[np.isnan(self.labels[stratum_id])]

        loc = np.random.choice(points)

        return loc

    def sample(self, prob_dist = None, replace = True):
        """
        Sample a datapoint

        Input
        -----
        prob_dist : float numpy array of length K (number of strata), optional
            probability distribution to use when sampling from the strata. If
            None, use the stratum weights.

        Output
        ------
        stratum_id : integer
            the randomly selected stratum index

        loc : integer
            the randomly selected location in the stratum
        """
        stratum_id = self._sample_stratum(prob_dist)

        loc = self._sample_in_stratum(stratum_id, replace = replace)

        return stratum_id, loc

    def update_label(self, stratum_id, loc, label):
        """

        """
        self.labels[stratum_id][loc] = label

    def split(self, st_id, loc, label):
        """
        Splits the stratum with index `st_id` after an anomalous point is
        sampled at location `loc` with label `label`.
        """
        # Check whether it is possible to split the stratum
        if self.populations[st_id] <= 1:
            warnings.warn("Cannot split stratum {}".format(st_id))
            return

        # Create a new empty stratum
        new_st_id = self._add_stratum()

        # Find locations of labelled points
        unlabelled = np.isnan(self.labels[st_id])
        # Exclude the recently sampled point from the labelled set, since
        # it will not remain in the old stratum
        labelled = ~unlabelled
        labelled[loc] = False
        labelled_loc = np.where(labelled)[0]
        unlabelled_loc = np.where(unlabelled)[0]

        n = self.populations[st_id]
        n_unlabelled = len(unlabelled_loc)
        n_old = np.ceil(n_unlabelled/2).astype(int)
        n_new = n_unlabelled - n_old

        # Split using scores -- try to keep points with lower scores with
        # labelled zeros, and points with higher scores with labelled ones.
        if label == 1:
            loc_old = np.append(labelled_loc, unlabelled_loc[0:n_old])
            loc_new = np.append(loc, unlabelled_loc[n_old:n_unlabelled])
        else:
            # label == 0
            loc_old = np.append(labelled_loc, unlabelled_loc[n_new:n_unlabelled])
            loc_new = np.append(loc, unlabelled_loc[0:n_new])

        # Preserve ordering (sorted by score in ascending order)
        loc_old = np.sort(loc_old)
        loc_new = np.sort(loc_new)

        self.allocations[new_st_id] = self.allocations[st_id][loc_new]
        self.populations[new_st_id] = n_new + 1
        self.weights[new_st_id] = self.populations[new_st_id]/self.num_pts
        self.labels[new_st_id] = self.labels[st_id][loc_new]
        self.scores[new_st_id] = self.scores[st_id][loc_new]
        self.preds[new_st_id] = self.preds[st_id][loc_new]

        self.allocations[st_id] = self.allocations[st_id][loc_old]
        self.populations[st_id] = n - n_unlabelled - 1 + n_old
        self.weights[st_id] = self.populations[st_id]/self.num_pts
        self.labels[st_id] = self.labels[st_id][loc_old]
        self.scores[st_id] = self.scores[st_id][loc_old]
        self.preds[st_id] = self.preds[st_id][loc_old]

    def calc_prior(self, strength = None):
        """
        Input
        -----
        strength : float, optional, default to number of strata
            strength of prior -- can be interpreted as the number of pseudo-
            observations

        Output
        ------
        alpha_0 : float numpy array of length K
            "alpha" hyperparameter for a sequence of K Beta-distributed rvs

        beta_0 : float numpy array of length K
            "alpha" hyperparameter for a sequence of K Beta-distributed rvs
        """

        if strength is None:
            strength = self.num_st

        weighted_strength = self.weights * strength

        if not self.calibrated_score:
            probs = []
            for k in self.indices:
                # TODO Fix scaling of logistic function
                probs.append(expit(self.scores[k] - self.score_threshold))
        else:
            probs = self.scores
        
        theta_0 = np.array([np.mean(x) for x in probs])
        alpha_0 = theta_0 * weighted_strength
        beta_0 = (1 - theta_0) * weighted_strength

        return alpha_0, beta_0

class BetaBernoulliModel:
    """

    """
    def __init__(self, alpha_0=None, beta_0=None, size = None):
        """
        Must provide size (Haldane prior) or alpha_0 and beta_0
        alpha_0 : numpy array of length `size`

        beta_0 : numpy array of length `size`

        size : integer
        """
        if size is None and alpha_0 is None and beta_0 is None:
            raise Exception("No valid arguments given. Must provide alpha_0 \
                             and beta_0, or size of model.")
        if size is None:
            # Ignore given size
            self.size = len(alpha_0)
        else:
            self.size = size
        if alpha_0 is None or beta_0 is None:
            # Approximate Haldane prior
            self.alpha_0 = np.repeat(np.finfo(float).eps, self.size)
            self.beta_0 = np.repeat(np.finfo(float).eps, self.size)
        else:
            # Use prior given
            self.alpha_0 = copy.deepcopy(alpha_0)
            self.beta_0 = copy.deepcopy(beta_0)
            if len(alpha_0) != len(beta_0):
                raise ValueError("alpha_0 and beta_0 have inconsistent lengths")

        # Number of positive labels sampled in each stratum (ignoring prior)
        self.alpha = np.zeros(self.size, dtype=int)
        # Number of negative labels sampled in each stratum (ignoring prior)
        self.beta = np.zeros(self.size, dtype=int)

        # Estimate of fraction of positive labels in each stratum (will
        # incorporate prior)
        self.theta = np.empty(self.size, dtype=float)
        # Estimate of variance in theta
        self.var_theta = np.empty(self.size, dtype=float)

        # Initialise
        self._calc_theta()
        self._calc_var_theta()

    def _calc_theta(self):
        # Include prior
        alpha = self.alpha + self.alpha_0
        beta = self.beta + self.beta_0
        # Mean of Beta-distributed rv
        self.theta = alpha / (alpha + beta)

    def _calc_var_theta(self):
        # Include prior
        alpha = self.alpha + self.alpha_0
        beta = self.beta + self.beta_0
        # Variance of Beta-distributed rv
        self.var_theta = alpha * beta / ((alpha + beta)**2 * (alpha + beta + 1))

    def update(self, y, k):
        """
        Updates the Beta-Bernoulli model given a point sampled from stratum `k`
        with label `y`.
        """
        self.alpha[k] = self.alpha[k] + y
        self.beta[k] = self.beta[k] + 1 - y

        self._calc_theta()
        self._calc_var_theta()

    def split(self, k_splt, pop_splt, pop_new):
        """
        k_splt : integer
            stratum index to split

        pop_splt : integer
            population of stratum `k_splt` after the split

        pop_new : integer
            population of newly-created stratum after the split
        """
        # Increase length of arrays by 1
        self.alpha = np.append(self.alpha, 0)
        self.beta = np.append(self.beta, 0)
        self.alpha_0 = np.append(self.alpha_0, 0)
        self.beta_0 = np.append(self.beta_0, 0)
        self.theta = np.append(self.theta, 0)
        self.var_theta = np.append(self.var_theta, 0)
        self.size += 1

        k_new = self.size - 1
        pop_tot = pop_splt + pop_new

        # Redistribute prior proportionately
        self.alpha_0[k_new] = self.alpha_0[k_splt] * pop_new/pop_tot
        self.alpha_0[k_splt] = self.alpha_0[k_splt] * pop_splt/pop_tot
        self.beta_0[k_new] = self.beta_0[k_splt] * pop_new/pop_tot
        self.beta_0[k_splt] = self.beta_0[k_splt] * pop_splt/pop_tot

        self._calc_theta()
        self._calc_var_theta()


class AOAIW:
    # TODO Fix the documentation
    """
    Input
    -----
    labels : int numpy array of length n
        array containing binary labels (assumed to be `0` or `1`) for each
        data point.

    strata: an instance of the `Strata` class
        contains information about the data points (allocations, scores)
        and facilitates sampling from the strata.

    alpha : float, optional, default 0.5
        weight to use for the F-measure. It should be between 0 and 1, with a
        value of 1 corresponding to "precision", a value of 0 corresponding to
        "recall" and a value of 0.5 corresponding to the balanced F-measure
        (equal weight on precision and recall). Note that this parameterisation
        of the weighted F-measure is different to the usual one (see the paper
        cited above for the definition).

    max_iter : int, optional, default None
        maximum number of iterations permitted (used to pre-allocate arrays for
        storing the sampling history). Defaults to the number of data points.

    prob_threshold :  double on the unit interval. Indicates the threshold to use
              for classification. If prob >= prob_threshold, the point is classified as 1,
              otherwise, the point is classified as 0. (default: 0.5)

    prior_strength : double

    epsilon : float, optional, default 1e-3
        epsilon-greedy parameter. Takes a value on the closed unit interval.
        The "optimal" distribution is used with probability `1 - epsilon`, and
        the passive distribution is used with probability `epsilon`. The
        sampling is close to "optimal" for small epsilon.

    debug : bool, optional, default False
        if True, prints debugging information.

    """
    def __init__(self, labels, strata, alpha = 0.5,
                   prob_threshold = 0.5, epsilon = 1e-3, prior_strength = None,
                   max_iter = None, debug = False):
        self._original_strata = strata
        self.strata = copy.deepcopy(strata)
        self.debug = debug
        self.labels = labels
        self.alpha = alpha
        self.prob_threshold = prob_threshold
        self.prior_strength = prior_strength
        self.epsilon = epsilon
        self.t = 0
        self._max_iter = max_iter
        self._F_prob_est = None

        if self._max_iter is None:
            self._max_iter = self.strata.num_pts

        # Terms used to calculate F-measure (we update them iteratively)
        self._TP_term = 0
        self._PP_term = 0
        self._P_term = 0

        # Instantiate Beta-Bernoulli model
        self.BB_model = \
                BetaBernoulliModel(*self.strata.calc_prior(self.prior_strength))

        # Array to record history of F-measure estimates
        self.F = np.repeat(np.nan, self._max_iter)

        # Array to record history of instrumental distributions
        self.pmf = np.zeros(self.strata.num_st, dtype=float)

        # Array to record whether oracle was queried at each iteration
        self.queried_oracle = np.repeat(False, self._max_iter)

    def _update_F_terms(self, y, yhat, w):
        """
        Iteratively update the terms that are used to calculate the F-measure
        after a new point is sampled with weight `w`, label `y` and prediction
        `yhat`.
        """
        if y == 1 and yhat == 1:
            # Point is true positive
            self._TP_term = self._TP_term + w
            self._PP_term = self._PP_term + w
            self._P_term = self._P_term + w
        elif yhat == 1:
            # Point is false positive
            self._PP_term = self._PP_term + w
        elif y == 1:
            # Point is false negative
            self._P_term = self._P_term + w

    def _update_F(self):
        """
        Records the latest estimate of the F-measure
        """

        t = self.t

        num = self._TP_term
        den = (self.alpha * self._PP_term + (1 - self.alpha) * self._P_term)

        if den == 0:
            self.F[t] = np.nan
        else:
            self.F[t] = num/den

    def _query_label(self, sample_id):
        """
        Queries the oracle for the label of the datapoint with id `sample_id`.
        Also records that the oracle was queried.

        Returns the ground truth label `0` or `1`.
        """

        t = self.t

        # Get label
        y = self.labels[sample_id]

        # Record that label was queried in this iteration
        self.queried_oracle[t] = True

        return y

    def _calc_F_prob_est(self):
        """
        Calculates and estimate of the F-measure based on the Beta-Bernoulli
        model and stores the result in self._F_prob_est
        """

        theta = self.BB_model.theta
        weights = self.strata.weights
        alpha = self.alpha
        predictions = (theta > self.prob_threshold) * 1
        F_num = np.sum(theta * weights * predictions)
        F_den = np.sum(theta * weights * (1 - alpha) + \
                       alpha * predictions * weights)
        self._F_prob_est = F_num/F_den

    def _calc_AOAIW_dist(self):
        """
        """
        # Easy vars
        epsilon = self.epsilon
        alpha = self.alpha
        thres = self.prob_threshold
        weights = self.strata.weights
        t = self.t

        # Use most recent estimates of F and theta
        F_est = np.nan if t == 0 else self.F[t - 1]
        p1 = self.BB_model.theta
        p0 = 1 - p1

        # Use an estimate for the F-measure based on the probs if it is np.nan
        if np.isnan(F_est) or F_est == 0:
            if self._F_prob_est is None:
                self._calc_F_prob_est()
            if self._F_prob_est < 1e-10:
                F_est = 1e-10
            else:
                F_est = self._F_prob_est

        # Predict positive pmf
        pp_pmf = ( weights * np.sqrt((alpha**2 *
                                    F_est**2 * p0 + (1 - F_est)**2 * p1)) )

        # Predict negative pmf
        pn_pmf = ( weights * (1 - alpha) * F_est * np.sqrt(p1) )

        self.pmf = (p1 >= thres) * pp_pmf + (p1 < thres) * pn_pmf
        # Normalize
        self.pmf = self.pmf/np.sum(self.pmf)
        # Weight by passive pmf
        self.pmf = epsilon * weights + (1 - epsilon) * self.pmf

    def reset(self):
        """
        Resets the instance to begin sampling again
        """
        self.t = 0

        self.strata = copy.deepcopy(self._original_strata)

        self._TP_term = 0
        self._PP_term = 0
        self._P_term = 0

        self.BB_model = \
                BetaBernoulliModel(*self.strata.calc_prior(self.prior_strength))

        self.F = np.repeat(np.nan, self._max_iter)

        # Array to record history of instrumental distributions
        self.pmf = np.zeros(self.strata.num_st, dtype=float)
        self.queried_oracle = np.repeat(False, self._max_iter)

    def sample(self, n_iter):
        """
        Samples `n_iter` points
        """

        t_i = self.t
        t_f = n_iter + self.t

        assert t_f <= self.F.shape[0]

        for t in range(t_i, t_f):
            # Calculate pmf
            self._calc_AOAIW_dist()

            # Sample label and record weight
            stratum_idx, sample_loc = self.strata.sample(prob_dist = self.pmf)
            w = self.strata.weights[stratum_idx]/self.pmf[stratum_idx]

            # Check if label has already been queried and stored
            y = self.strata.labels[stratum_idx][sample_loc]
            if np.isnan(y):
                # Need to query oracle
                sample_idx = self.strata.allocations[stratum_idx][sample_loc]
                y = self._query_label(sample_idx)
                # Store label
                self.strata.update_label(stratum_idx, sample_loc, y)

            # Get score
            pred = self.strata.preds[stratum_idx][sample_loc]

            if self.debug == True:
                print("TP_term: {}, PP_term: {}, P_term: {}".format(self._TP_term, self._PP_term, self._P_term))
                print("Sampled label {} for point {} in stratum {}. Weight is {}.".format(y,self.strata.allocations[stratum_idx][sample_loc], stratum_idx, w))


            if ( self.strata.splitting and
                 ( ( y == 1 and self.BB_model.alpha[stratum_idx] == 0 and
                     self.BB_model.beta[stratum_idx] > 0 ) or
                   ( y == 0 and self.BB_model.beta[stratum_idx] == 0 and
                     self.BB_model.alpha[stratum_idx] > 0 ) ) and
                   self.strata.populations[stratum_idx] > 1 ):
                # Split if the sampled label is different from those previously
                # seen in the stratum
                if self.debug:
                    print("Splitting stratum {}".format(stratum_idx))

                # Split stratum
                self.strata.split(stratum_idx, sample_loc, y)

                # Get populations of split stratum and new stratum
                pop_splt = self.strata.populations[stratum_idx]
                pop_new = self.strata.populations[-1]

                # Fix BB model
                self.BB_model.split(stratum_idx, pop_splt, pop_new)

                # Change stratum_idx so that new stratum will be updated below
                stratum_idx = self.strata.indices[-1]

                # Increase size of pmf array
                self.pmf = np.append(self.pmf, 0)

            self.BB_model.update(y, stratum_idx)
            self._update_F_terms(y, pred, w)
            self._update_F()

            self.t = self.t + 1

    def sample_until(self, n_goal):
        """
        Sample until `n_goal` labels are queried from the oracle
        """

        n_seen = np.sum(self.queried_oracle)

        if n_seen >= n_goal:
            print("Have already queried {} labels from the oracle".format(n_seen))
            return

        if n_goal > self._max_iter:
            print("{} is greater than max_iter = {}".format(n_goal,self._max_iter))
            return

        while n_seen < n_goal:
            self.sample(1)
            n_seen = n_seen + self.queried_oracle[self.t - 1]*1

class Kadane:
    """
    Input
    -----
    labels : int numpy array of length n
        array containing binary labels (assumed to be `0` or `1`) for each
        data point.

    strata: an instance of the `Strata` class
        contains information about the data points (allocations, scores)
        and facilitates sampling from the strata.

    alpha : float, optional, default 0.5
        weight to use for the F-measure. It should be between 0 and 1, with a
        value of 1 corresponding to "precision", a value of 0 corresponding to
        "recall" and a value of 0.5 corresponding to the balanced F-measure
        (equal weight on precision and recall). Note that this parameterisation
        of the weighted F-measure is different to the usual one (see the paper
        cited above for the definition).

    n_initial : int, optional, default 0
        sample `n_initial` labels uniformly from each stratum before starting
        the epsilon-greedy strategy.

    frac : double, optional, default 0.01
        make the prior decay to 1/e in strength after a fraction `frac` of the
        total labels are sampled.

    epsilon : float, optional, default 1e-3
        epsilon-greedy parameter. Takes a value on the closed unit interval.
        The "optimal" distribution is used with probability `1 - epsilon`, and
        the passive distribution is used with probability `epsilon`. The
        sampling is close to "optimal" for small epsilon.

    debug : bool, optional, default False
        if True, prints debugging information.
    """
    def __init__(self, labels, strata, alpha = 0.5, n_initial = 3,
                 epsilon = 0.001, frac = 0.01, debug = False):
        # TODO variance
        self._original_strata = strata
        self.labels = labels
        self.strata = copy.deepcopy(strata)
        self.alpha = alpha
        self.t = 0
        self.num_pts = len(labels)
        self.num_st = self.strata.num_st
        self.n_initial = n_initial
        self.epsilon = epsilon
        self.debug = debug
        self.frac = frac

        self.n_remaining = copy.deepcopy(self.strata.populations)
        self.n_sampled = np.zeros(self.num_st, dtype=int)

        self._TP_term = 0
        self._PP_term = 0
        self._P_term = 0

        # Arrays for recording the number of TP/PP/P in each stratum
        self._TP = np.zeros(self.num_st, dtype=float)
        self._PP = np.zeros(self.num_st, dtype=float)
        self._P = np.zeros(self.num_st, dtype=float)

        self.cov_prior = np.zeros([3,3,self.num_st])
        self.F_prior = 0
        self.decrease_prior = np.zeros(self.num_st, dtype=float)
        self._calc_prior()

        # Arrays for recording the values of `t`, `p` and `y`
        self.ts = [np.repeat(np.nan, x) for x in self.strata.populations]
        self.ps = [np.repeat(np.nan, x) for x in self.strata.populations]
        self.ys = [np.repeat(np.nan, x) for x in self.strata.populations]

        # Array to store history of F-measure estimates
        self.F = np.repeat(np.nan, self.num_pts)
        self.cov = np.zeros([3,3,self.num_st])

        # Array to store history of variance decrease
        self.decrease = np.zeros([self.num_st, self.num_pts], dtype=float)

        self.pmf = np.zeros([self.num_st, self.num_pts], dtype=float)
        self.greedy_pmf = np.zeros([self.num_st, self.num_pts], dtype=float)

        # Sample a fixed number of points from each stratum
        for _ in range(self.n_initial):
            for k in self.strata.indices:
                self.sample(1, k)

    def _calc_prior(self):
        """

        """
        populations = self.strata.populations
        weights = self.strata.weights
        alpha = self.alpha

        if self.strata.calibrated_score:
            Y_bar_est = np.array([np.mean(x) for x in self.strata.scores])
        else:
            Y_bar_est = np.array([np.mean(expit(x - self.strata.score_threshold)) for x in self.strata.scores])
        P_bar_est = np.array([np.mean((x >= self.strata.score_threshold) * 1) for x in self.strata.scores])
        T_bar_est = Y_bar_est * P_bar_est

        factor = populations/(populations - 1)
        self.cov_prior[0,0,:] = factor * T_bar_est * (1 - T_bar_est)
        self.cov_prior[1,1,:] = factor * P_bar_est * (1 - P_bar_est)
        self.cov_prior[2,2,:] = factor * Y_bar_est * (1 - Y_bar_est)
        self.cov_prior[0,1,:] = factor * T_bar_est * (1 - P_bar_est)
        self.cov_prior[0,2,:] = factor * T_bar_est * (1 - Y_bar_est)
        self.cov_prior[1,2,:] = factor * T_bar_est - P_bar_est * Y_bar_est
        self.cov_prior[1,0,:] = self.cov_prior[0,1,:]
        self.cov_prior[2,0,:] = self.cov_prior[0,2,:]
        self.cov_prior[2,1,:] = self.cov_prior[1,2,:]

        F_num = np.dot(weights, T_bar_est)
        F_den = ( alpha * np.dot(weights, P_bar_est) +
                  (1 - alpha) * np.dot(weights, Y_bar_est) )
        self.F_prior = F_num/F_den

        self.decrease_prior = self._calc_decrease(self.cov_prior, self.F_prior)

    def _update_F_terms(self, y, pred, sample_loc, stratum_idx):
        """
        Iteratively update the terms that are used to calculate the F-measure
        after a new point is sampled from stratum `stratum_idx`, at location
        `sample_loc` with label `y` and predicition `pred`.
        """
        t = self.t
        weights = self.strata.weights
        populations = self.strata.populations
        n_sampled = np.clip(self.n_sampled, 1e-10, np.inf)

        self.ps[stratum_idx][sample_loc] = pred
        self._PP[stratum_idx] += pred
        self.ys[stratum_idx][sample_loc] = y
        self._P[stratum_idx] += y
        if y == 1 and pred == 1:
            self.ts[stratum_idx][sample_loc] = 1
            self._TP[stratum_idx] += 1
        else:
            self.ts[stratum_idx][sample_loc] = 0

        notnan = ~np.isnan(self.ts[stratum_idx])
        if np.sum(notnan) >= 2:
            self.cov[:,:,stratum_idx] = np.cov(np.array([self.ts[stratum_idx][notnan], \
                       self.ps[stratum_idx][notnan], self.ys[stratum_idx][notnan]]))

        self._TP_term = np.dot(weights, self._TP/n_sampled)
        self._PP_term = np.dot(weights, self._PP/n_sampled)
        self._P_term = np.dot(weights, self._P/n_sampled)

    def _update_F(self):
        """
        Records the latest estimate of the F-measure
        """
        t = self.t

        num = self._TP_term
        den = (self.alpha * self._PP_term + (1 - self.alpha) * self._P_term)

        if den == 0:
            self.F[t] = np.nan
        else:
            self.F[t] = num/den

    def _calc_decrease(self, cov, F):
        alpha = self.alpha
        populations = self.strata.populations
        # Note adding 1 as a "pseudo-count" -- fix issue with division by zero
        n_sampled = self.strata.populations - self.n_remaining
        return ( ( cov[0,0,:]**2 - 2 * F * (alpha * cov[0,1,:]**2 + (1 - alpha) * cov[0,2,:]**2)
                     + F**2 * ( (1 - alpha)**2 * cov[2,2,:]**2 + alpha**2 * cov[1,1,:]**2
                                 + 2 * (1 - alpha) * alpha * cov[1,2,:]**2 ) ) *
                   populations**2 ) / (n_sampled * (n_sampled + 1) + 1)

    def _calc_pmf(self):
        """
        """
        t = self.t

        weights = self.strata.weights
        epsilon = self.epsilon
        eta = 1/(self.frac * self.num_pts)
        F = self.F[t - 1]
        if np.isnan(F):
            F = self.F_prior

        self.decrease[:,t] = self.decrease_prior * np.exp(-eta * t)

        temp_decrease = self._calc_decrease(self.cov, F) * (1 - np.exp(-eta * t))
        #temp_decrease[self.n_sampled <= 2] = 0

        self.decrease[:,t] += temp_decrease
        self.decrease[self.n_sampled < 2, t] = self.decrease_prior[self.n_sampled < 2]

        nonempty = (self.n_remaining > 0)
        # Among non-empty strata, sort according to "variance decrease" in decreasing order
        sorted_decrease = np.sort(self.decrease[nonempty,t])[::-1]
        # Select stratum which gives largest decrease (allowing for ties)
        best_ids = np.where((self.decrease[:,t] == sorted_decrease[0]) & nonempty)[0]
        if self.debug:
            print("Best ids: {}".format(best_ids))
        self.greedy_pmf[best_ids,t] = 1/len(best_ids)

        # Sample uniformly from non-empty strata
        self.pmf[:,t] = weights * nonempty
        self.pmf[:,t] = self.pmf[:,t]/np.sum(self.pmf[:,t])
        self.pmf[:,t] = epsilon * self.pmf[:,t] + (1 - epsilon) * self.greedy_pmf[:,t]

    def reset(self):
        self.t = 0

        self.strata = copy.deepcopy(self._original_strata)

        self.n_remaining = copy.deepcopy(self.strata.populations)
        self.n_sampled = np.zeros(self.num_st, dtype=int)

        self._TP_term = 0
        self._PP_term = 0
        self._P_term = 0

        self._TP = np.zeros(self.num_st, dtype=float)
        self._PP = np.zeros(self.num_st, dtype=float)
        self._P = np.zeros(self.num_st, dtype=float)

        self.ts = [np.repeat(np.nan, x) for x in self.strata.populations]
        self.ps = [np.repeat(np.nan, x) for x in self.strata.populations]
        self.ys = [np.repeat(np.nan, x) for x in self.strata.populations]

        self.F = np.repeat(np.nan, self.num_pts)
        self.cov = np.zeros([3,3,self.num_st])

        self.decrease = np.zeros([self.num_st, self.num_pts], dtype=float)

        self.pmf = np.zeros([self.num_st, self.num_pts], dtype=float)
        self.greedy_pmf = np.zeros([self.num_st, self.num_pts], dtype=float)

        for _ in range(self.n_initial):
            for k in self.strata.indices:
                self.sample(1, k)

    def sample(self, n_iter, fixed_stratum = None):
        """
        Samples `n_iter` points
        fixed_stratum :

        """

        t_i = self.t
        t_f = n_iter + self.t

        # TODO Replace this assertion
        assert t_f <= self.F.shape[0]

        for t in range(t_i, t_f):
            # Check if there are any more points to sample
            if np.sum(self.n_remaining) == 0:
                print("All points have been sampled")
                return

            # Sample point
            if fixed_stratum is not None:
                stratum_idx = fixed_stratum
                sample_loc = self.strata._sample_in_stratum(stratum_idx, replace = False)
            else:
                self._calc_pmf()
                stratum_idx, sample_loc = self.strata.sample(prob_dist = self.pmf[:,self.t], replace = False)

            self.n_sampled[stratum_idx] +=1

            # Query label from oracle
            sample_idx = self.strata.allocations[stratum_idx][sample_loc]
            y = self.labels[sample_idx]
            self.strata.update_label(stratum_idx, sample_loc, y)

            # Get score
            pred = self.strata.preds[stratum_idx][sample_loc]

            self._update_F_terms(y, pred, sample_loc, stratum_idx)
            self._update_F()
            self.n_remaining[stratum_idx] -= 1
            self.t += 1

            if self.debug == True:
                print("TP_term: {}, PP_term: {}, P_term: {}".format(self._TP_term, self._PP_term, self._P_term))
                print("Sampled label {} for point {} in stratum {}.".format(y, self.strata.allocations[stratum_idx][sample_loc], stratum_idx))

    def sample_until(self, n_goal):
        """
        Sample until `n_goal` labels are queried from the oracle
        """

        n_seen = self.t

        if n_seen >= n_goal:
            print("Have already queried {} labels from the oracle".format(n_seen))
            return

        if n_goal > self.num_pts:
            print("{} is greater than the number of points in the dataset".format(n_goal))
            return

        while n_seen < n_goal:
            self.sample(1)
            n_seen +=1
